{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from SMT import SMT\n",
    "from config_typings import Config, SMTConfig, DataConfig, CLConfig\n",
    "from data import PretrainingLinesDataset\n",
    "from torchinfo import summary\n",
    "from utils import check_and_retrieveVocabulary\n",
    "from data_augmentation.data_augmentation import augment, convert_img_to_tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from model.ConvEncoder import Encoder\n",
    "from model.ConvNextEncoder import ConvNextEncoder\n",
    "from model.Decoder import Decoder\n",
    "from model.PositionEncoding import PositionalEncoding2D, PositionalEncoding1D\n",
    "\n",
    "from eval_functions import compute_poliphony_metrics\n",
    "from Generator.MusicSynthGen import VerovioGenerator\n",
    "\n",
    "from rich import progress\n",
    "\n",
    "import lightning.pytorch as L\n",
    "from lightning.pytorch import Trainer, LightningDataModule\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigSection:\n",
    "    def __init__(self, section_dict):\n",
    "        for key, value in section_dict.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, config_dict):\n",
    "        for key, value in config_dict.items():\n",
    "            setattr(self, key, ConfigSection(value) if isinstance(value, dict) else value)\n",
    "\n",
    "# Now you can create an instance of the Config class with your dictionary\n",
    "config_dict = {\n",
    "    'data': {\n",
    "        'data_path': 'Data/GrandStaff/partitions_grandstaff/types/', \n",
    "        'synth_path': '', \n",
    "        'vocab_name': 'GrandStaff_BeKern', \n",
    "        'out_dir': 'out/GrandStaff/SMT_lines', \n",
    "        'krn_type': 'bekrn', \n",
    "        'reduce_ratio': 0.5, \n",
    "        'base_folder': 'GrandStaff', \n",
    "        'file_format': 'jpg', \n",
    "        'tokenization_mode': 'bekern', \n",
    "        'fold': '???'\n",
    "    },\n",
    "    'model_setup': {\n",
    "        'in_channels': 1, \n",
    "        'd_model': 256, \n",
    "        'dim_ff': 256, \n",
    "        'num_dec_layers': 8, \n",
    "        'encoder_type': 'NexT', \n",
    "        'max_height': 2512, \n",
    "        'max_width': 2512, \n",
    "        'max_len': 5512, \n",
    "        'lr': 0.001\n",
    "    }, \n",
    "    'experiment': {\n",
    "        'metric_to_watch': 'val_SER', \n",
    "        'metric_mode': 'min', \n",
    "        'max_epochs': 100, \n",
    "        'val_after': 5, \n",
    "        'pretrain_weights': '???'\n",
    "    }, \n",
    "    'cl': {\n",
    "        'num_cl_steps': 3, \n",
    "        'max_synth_prob': 0.9, \n",
    "        'min_synth_prob': 0.2, \n",
    "        'increase_steps': 40000, \n",
    "        'finetune_steps': 200000, \n",
    "        'curriculum_stage_beginning': 2, \n",
    "        'teacher_forcing_perc': '???', \n",
    "        'skip_progressive': '???', \n",
    "        'skip_cl': '???'\n",
    "    }\n",
    "}\n",
    "\n",
    "config = Config(config_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SMT(L.LightningModule):\n",
    "    def __init__(self, config:SMTConfig, w2i, i2w) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if config.encoder_type == \"NexT\":\n",
    "            self.encoder = ConvNextEncoder(in_chans=config.in_channels, depths=[3,3,9], dims=[64, 128, 256])\n",
    "        else:\n",
    "            self.encoder = Encoder(in_channels=config.in_channels)\n",
    "\n",
    "        self.decoder = Decoder(config.d_model, config.dim_ff, config.num_dec_layers, config.max_len + 1, len(w2i))\n",
    "        self.positional_2D = PositionalEncoding2D(config.d_model, (config.max_height//16) + 1, (config.max_width//8) + 1)\n",
    "\n",
    "        self.padding_token = 0\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=self.padding_token)\n",
    "\n",
    "        self.valpredictions = []\n",
    "        self.valgts = []\n",
    "\n",
    "        self.w2i = w2i\n",
    "        self.i2w = i2w\n",
    "\n",
    "        self.maxlen = config.max_len\n",
    "\n",
    "        #self(torch.randn(1,1,config.max_height,config.max_width).to(torch.device(\"cuda\")), torch.randint(0, len(w2i), (1,config.max_len)).to(torch.device(\"cuda\")))\n",
    "        #import sys\n",
    "        #sys.exit()\n",
    "        self.worst_loss_image = None\n",
    "        self.worst_training_loss = -1\n",
    "        summary(self, input_size=[(1,1, 2512, 2512), (1, 5512)], dtypes=[torch.float, torch.long])\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, y_pred):\n",
    "        encoder_output = self.encoder(x)\n",
    "        b, c, h, w = encoder_output.size()\n",
    "        reduced_size = [s.shape[:2] for s in encoder_output]\n",
    "        ylens = [len(sample) for sample in y_pred]\n",
    "        cache = None\n",
    "\n",
    "        pos_features = self.positional_2D(encoder_output)\n",
    "        features = torch.flatten(encoder_output, start_dim=2, end_dim=3).permute(2,0,1)\n",
    "        enhanced_features = features\n",
    "        enhanced_features = torch.flatten(pos_features, start_dim=2, end_dim=3).permute(2,0,1)\n",
    "        output, predictions, _, _, weights = self.decoder(features, enhanced_features, y_pred[:, :-1], reduced_size, \n",
    "                                                           [max(ylens) for _ in range(b)], encoder_output.size(), \n",
    "                                                           start=0, cache=cache, keep_all_weights=True)\n",
    "    \n",
    "        return output, predictions, cache, weights\n",
    "\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def forward_decoder(self, encoder_output, last_preds, cache=None):\n",
    "        b, c, h, w = encoder_output.size()\n",
    "        reduced_size = [s.shape[:2] for s in encoder_output]\n",
    "        ylens = [len(sample) for sample in last_preds]\n",
    "        cache = cache\n",
    "\n",
    "        pos_features = self.positional_2D(encoder_output)\n",
    "        features = torch.flatten(encoder_output, start_dim=2, end_dim=3).permute(2,0,1)\n",
    "        enhanced_features = features\n",
    "        enhanced_features = torch.flatten(pos_features, start_dim=2, end_dim=3).permute(2,0,1)\n",
    "        output, predictions, _, _, weights = self.decoder(features, enhanced_features, last_preds[:, :], reduced_size, \n",
    "                                                           [max(ylens) for _ in range(b)], encoder_output.size(), \n",
    "                                                           start=0, cache=cache, keep_all_weights=True)\n",
    "    \n",
    "        return output, predictions, cache, weights\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=2e-4, amsgrad=False)\n",
    "\n",
    "    def training_step(self, train_batch):\n",
    "        x, di, y = train_batch\n",
    "        output, predictions, cache, weights = self.forward(x, di)\n",
    "        loss = self.loss(predictions, y[:, :-1])\n",
    "        self.log('loss', loss, on_epoch=True, batch_size=1, prog_bar=True)\n",
    "        if loss > self.worst_training_loss:\n",
    "            self.worst_loss_image = x\n",
    "            self.worst_training_loss = loss\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        #plot the worst training loss image in wandb\n",
    "        self.logger.experiment.log({\"worst_training_loss_image\": [wandb.Image(self.worst_loss_image.squeeze(0).cpu().numpy())]})\n",
    "        self.worst_training_loss = -1\n",
    "        self.worst_loss_image = None\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        x, _, y = val_batch\n",
    "        encoder_output = self.forward_encoder(x)\n",
    "        predicted_sequence = torch.from_numpy(np.asarray([self.w2i['<bos>']])).to(device).unsqueeze(0)\n",
    "        cache = None\n",
    "        for i in range(128):\n",
    "             output, predictions, cache, weights = self.forward_decoder(encoder_output, predicted_sequence.long(), cache=cache)\n",
    "             predicted_token = torch.argmax(predictions[:, :, -1]).item()\n",
    "             predicted_sequence = torch.cat([predicted_sequence, torch.argmax(predictions[:, :, -1], dim=1, keepdim=True)], dim=1)\n",
    "             if predicted_token == self.w2i['<eos>']:\n",
    "                 break\n",
    "        \n",
    "        dec = \"\".join([self.i2w[token.item()] for token in predicted_sequence.squeeze(0)[1:]])\n",
    "        dec = dec.replace(\"<t>\", \"\\t\")\n",
    "        dec = dec.replace(\"<b>\", \"\\n\")\n",
    "        dec = dec.replace(\"<s>\", \" \")\n",
    "\n",
    "        gt = \"\".join([self.i2w[token.item()] for token in y.squeeze(0)[:-1]])\n",
    "        gt = gt.replace(\"<t>\", \"\\t\")\n",
    "        gt = gt.replace(\"<b>\", \"\\n\")\n",
    "        gt = gt.replace(\"<s>\", \" \")\n",
    "\n",
    "         # Write to file\n",
    "        with open(\"validation_results.txt\", \"a\") as f:\n",
    "            f.write(f\"[Prediction] - {dec}\\n\")\n",
    "            f.write(f\"[GT] - {gt}\\n\")\n",
    "\n",
    "        self.valpredictions.append(dec)\n",
    "        self.valgts.append(gt)\n",
    "    \n",
    "    def on_validation_epoch_end(self, name=\"val\"):\n",
    "        cer, ser, ler = compute_poliphony_metrics(self.valpredictions, self.valgts)\n",
    "        \n",
    "        random_index = np.random.randint(0, len(self.valpredictions))\n",
    "        predtoshow = self.valpredictions[random_index]\n",
    "        gttoshow = self.valgts[random_index]\n",
    "        print(f\"[Prediction] - {predtoshow}\")\n",
    "        print(f\"[GT] - {gttoshow}\")\n",
    "\n",
    "        self.log(f'{name}_CER', cer, prog_bar=True)\n",
    "        self.log(f'{name}_SER', ser, prog_bar=True)\n",
    "        self.log(f'{name}_LER', ler, prog_bar=True)\n",
    "\n",
    "        self.valpredictions = []\n",
    "        self.valgts = []\n",
    "\n",
    "        return ser\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        self.validation_step(test_batch, batch_idx)\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        return self.on_validation_epoch_end(name=\"test\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5560e98f55a4784b038683c6349d6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_set(path, base_folder=\"GrandStaff\", fileformat=\"jpg\", krn_type=\"bekrn\", reduce_ratio=0.5):\n",
    "    x = []\n",
    "    y = []\n",
    "    limit = 500000\n",
    "    counter = 0\n",
    "    with open(path) as datafile:\n",
    "        lines = datafile.readlines()\n",
    "        for line in progress.track(lines):\n",
    "            if counter > limit:\n",
    "                break\n",
    "            counter += 1\n",
    "            excerpt = line.replace(\"\\n\", \"\")\n",
    "            try:\n",
    "                with open(f\"Data/{base_folder}/{'.'.join(excerpt.split('.')[:-1])}.{krn_type}\") as krnfile:\n",
    "                    krn_content = krnfile.read()\n",
    "                    fname = \".\".join(excerpt.split('.')[:-1])\n",
    "                    img = cv2.imread(f\"Data/{base_folder}/{fname}.{fileformat}\")\n",
    "                    width = int(np.ceil(img.shape[1] * reduce_ratio))\n",
    "                    height = int(np.ceil(img.shape[0] * reduce_ratio))\n",
    "                    img = cv2.resize(img, (width, height))\n",
    "                    y.append([content + '\\n' for content in krn_content.strip().split(\"\\n\")])\n",
    "                    x.append(img)\n",
    "            except Exception as e:\n",
    "               print(f'Error reading Data/GrandStaff/{excerpt}')\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x, y = load_set(\"/home/ubuntu/maikyon/music-sheet-music-modeling/SMT-plusplus/Data/GrandStaff/partitions_grandstaff/types/train.txt\", base_folder=config.data.base_folder, \n",
    "                                  fileformat=config.data.file_format, krn_type=config.data.krn_type, reduce_ratio=config.data.reduce_ratio)\n",
    "def preprocess_gt(Y, tokenization_method=\"standard\"):\n",
    "    for idx, krn in enumerate(Y):\n",
    "        krnlines = []\n",
    "        krn = \"\".join(krn)\n",
    "        krn = krn.replace(\" \", \" <s> \")\n",
    "        \n",
    "        if tokenization_method == \"bekern\":\n",
    "            krn = krn.replace(\"·\", \" \")\n",
    "            krn = krn.replace(\"@\", \" \")\n",
    "        if tokenization_method == \"ekern\":\n",
    "            krn = krn.replace(\"·\", \" \")\n",
    "            krn = krn.replace(\"@\", \"\")\n",
    "        if tokenization_method == \"standard\":\n",
    "            krn = krn.replace(\"·\", \"\")\n",
    "            krn = krn.replace(\"@\", \"\")\n",
    "            \n",
    "        krn = krn.replace(\"/\", \"\")\n",
    "        krn = krn.replace(\"\\\\\", \"\")\n",
    "        krn = krn.replace(\"\\t\", \" <t> \")\n",
    "        krn = krn.replace(\"\\n\", \" <b> \")\n",
    "        krn = krn.split(\" \")\n",
    "                \n",
    "        Y[idx] = erase_numbers_in_tokens_with_equal(['<bos>'] + krn[4:-1] + ['<eos>'])\n",
    "    return Y\n",
    "def erase_numbers_in_tokens_with_equal(tokens):\n",
    "    return [re.sub(r'(?<=\\=)\\d+', '', token) for token in tokens]\n",
    "\n",
    "def erase_whitespace_elements(tokens):\n",
    "    return [token for token in tokens if token != \"\"]\n",
    "\n",
    "y = preprocess_gt(y, tokenization_method=config.data.tokenization_mode)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "def make_vocabulary(YSequences, nameOfVoc):\n",
    "    vocabulary = set()\n",
    "    for samples in YSequences:\n",
    "        for element in samples:\n",
    "            vocabulary.add(element)\n",
    "\n",
    "    #Vocabulary created\n",
    "    w2i = {symbol:idx+1 for idx,symbol in enumerate(vocabulary)}\n",
    "    i2w = {idx+1:symbol for idx,symbol in enumerate(vocabulary)}\n",
    "    \n",
    "    w2i['<pad>'] = 0\n",
    "    i2w[0] = '<pad>'\n",
    "\n",
    "\n",
    "    return w2i, i2w\n",
    "\n",
    "w2i, i2w = make_vocabulary(y, \"JVocab\")\n",
    "print(len(w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Utility Functions\n",
    "def erase_whitespace_elements(tokens):\n",
    "    return [token for token in tokens if token != \"\"]\n",
    "\n",
    "def batch_preparation_img2seq(data):\n",
    "    images = [sample[0] for sample in data]\n",
    "    dec_in = [sample[1] for sample in data]\n",
    "    gt = [sample[2] for sample in data]\n",
    "\n",
    "    max_image_width = max([img.shape[2] for img in images])\n",
    "    max_image_height = max([img.shape[1] for img in images])\n",
    "\n",
    "    X_train = torch.ones(size=[len(images), 1, max_image_height, max_image_width], dtype=torch.float32)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        _, h, w = img.size()\n",
    "        X_train[i, :, :h, :w] = img\n",
    "    \n",
    "    max_length_seq = max([len(seq) for seq in gt])\n",
    "\n",
    "    decoder_input = torch.zeros(size=[len(dec_in), max_length_seq], dtype=torch.long)\n",
    "    y = torch.zeros(size=[len(gt), max_length_seq], dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(dec_in):\n",
    "        seq_tensor = seq.clone().detach()\n",
    "        decoder_input[i, :len(seq_tensor)] = seq_tensor\n",
    "    \n",
    "    for i, seq in enumerate(gt):\n",
    "        seq_tensor = seq.clone().detach()\n",
    "        y[i, :len(seq_tensor)] = seq_tensor\n",
    "    \n",
    "    return X_train, decoder_input, y\n",
    "\n",
    "# Define worker_init_fn to set CPU affinity\n",
    "def worker_init_fn(worker_id):\n",
    "    os.sched_setaffinity(0, range(os.cpu_count()))\n",
    "\n",
    "# Dataset Class\n",
    "class OMRIMG2SEQDataset(Dataset):\n",
    "    def __init__(self, x, y, w2i, i2w, teacher_forcing_perc=0.2, augment=False) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w2i = w2i\n",
    "        self.i2w = i2w\n",
    "        self.teacher_forcing_error_rate = teacher_forcing_perc\n",
    "        self.augment = augment\n",
    "        self.padding_token = w2i['<pad>']\n",
    "        super().__init__()\n",
    "    \n",
    "    def apply_teacher_forcing(self, sequence):\n",
    "        errored_sequence = sequence.clone().detach()\n",
    "        for token in range(1, len(sequence)):\n",
    "            if np.random.rand() < self.teacher_forcing_error_rate and sequence[token] != self.padding_token:\n",
    "                errored_sequence[token] = np.random.randint(0, len(self.w2i))\n",
    "        return errored_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        x = convert_img_to_tensor(x)  # Define convert_img_to_tensor function as needed\n",
    "        y = [self.w2i[token] for token in erase_whitespace_elements(y)]\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        decoder_input = self.apply_teacher_forcing(y)\n",
    "        return x, decoder_input, y\n",
    "\n",
    "    @staticmethod\n",
    "    def erase_whitespace_elements(tokens):\n",
    "        return [token for token in tokens if token != \"\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def erase_numbers_in_tokens_with_equal(tokens):\n",
    "        return [re.sub(r'(?<=\\=)\\d+', '', token) for token in tokens]\n",
    "\n",
    "# DataModule Class\n",
    "class OMRDataModule(L.LightningDataModule):\n",
    "    def __init__(self, x, y, w2i, i2w, batch_size=32, augment=False, teacher_forcing_perc=0.2, num_workers=4, train_val_test_split=(0.9, 0.9, 0.01)):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w2i = w2i\n",
    "        self.i2w = i2w\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        self.teacher_forcing_perc = teacher_forcing_perc\n",
    "        self.num_workers = num_workers\n",
    "        self.train_val_test_split = train_val_test_split\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = OMRIMG2SEQDataset(\n",
    "            x=self.x,\n",
    "            y=self.y,\n",
    "            w2i=self.w2i,\n",
    "            i2w=self.i2w,\n",
    "            teacher_forcing_perc=self.teacher_forcing_perc,\n",
    "            augment=self.augment\n",
    "        )\n",
    "\n",
    "        train_size = int(self.train_val_test_split[0] * len(full_dataset))\n",
    "        val_size = int(self.train_val_test_split[1] * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        \n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=batch_preparation_img2seq, worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=batch_preparation_img2seq, worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=batch_preparation_img2seq, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module = PretrainingLinesDataset(config.data)\n",
    "data_module = OMRDataModule(x, y, w2i, i2w, batch_size=1, num_workers=30, augment=False)\n",
    "model = SMT(config=config.model_setup, w2i=w2i, i2w=i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                 | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | encoder       | ConvNextEncoder      | 5.5 M  | train\n",
      "1 | decoder       | Decoder              | 5.4 M  | train\n",
      "2 | positional_2D | PositionalEncoding2D | 0      | train\n",
      "3 | loss          | CrossEntropyLoss     | 0      | train\n",
      "---------------------------------------------------------------\n",
      "10.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.9 M    Total params\n",
      "43.660    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeecb4c107f4c058152e73e3ae45ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prediction] - XXXXXXXXXXXXXXXXXXXXXXXXXXXX=:|!XXXXXXXXXXXXXXXXXXXXXXXXXXX=:|!=:|!=:|!=:|!LL>=:|!LL>*k[b-]XXX*k[b-e-a-d-g-c-]X*k[b-e-a-d-g-c-]X*k[b-e-a-d-g-c-]X*k[b-e-a-d-g-c-]XXXXXX*k[b-e-a-d-g-c-]FFFFFFFFFFFFFFFFFFFFFFFFFFFFFF*k[b-]aaXXX*k[b-e-a-d-g-c-]X*k[b-e-a-d-g-c-]*k[f#c#g#d#a#]aaX=:|!=:|!=:|!*^X*k[b-e-a-d-g-c-]X*k[b-e-a-d-g-c-]X=:|!LL>*k[b-]*k[f#c#]X=:|!LL>=:|!LL>=:|!=:|!=:|!\n",
      "[GT] - <bos>*clefF4\t*clefG2\n",
      "*k[b-e-a-d-g-c-]\t*k[b-e-a-d-g-c-]\n",
      "*M44\t*M44\n",
      "*met(c)\t*met(c)\n",
      "=-\t=-\n",
      "8e-L\t4eee-\n",
      "8a- 8cc-\t.\n",
      "8a- 8cc-\t8r\n",
      "8a- 8cc-J\t8aaa-\n",
      "8e-L\t8ggg-L\n",
      "8a- 8cc- 8dd\t8fff\n",
      "8a- 8cc- 8dd\t8eee-\n",
      "8a- 8cc- 8ddJ\t8dddJ\n",
      "=\t=\n",
      "8e-L\t8fffL\n",
      "8g- 8b-\t8eee-J\n",
      "8g- 8b-\t8r\n",
      "8g- 8b-J\t8ee-\n",
      "8e-L\t20ddLL\n",
      ".\t20ee-\n",
      ".\t20ee\n",
      "8g 8b- 8dd-\t.\n",
      ".\t20ff\n",
      ".\t20gg-JJ\n",
      "8g 8b- 8dd-\t16gg-LL\n",
      ".\t16aa-\n",
      "8g 8b- 8dd-J\t16aa\n",
      ".\t16bb-JJ\n",
      "=\t=\n",
      "8e-L\t16ccc-LL\n",
      ".\t16ccc\n",
      "8a- 8cc-\t16ddd-\n",
      ".\t16dddJJ\n",
      "8a- 8cc-\t8eee-L\n",
      "8a- 8cc-J\t8aaa-J\n",
      "8e-L\t8ggg-L\n",
      "8a- 8cc- 8dd\t8fff\n",
      "8a- 8cc- 8dd\t8eee-\n",
      "8a- 8cc- 8ddJ\t8dddJ\n",
      "=\t=\n",
      "4e- 4g- 4b- 4ee-\t16eee-LL\n",
      ".\t16fff\n",
      ".\t16eee-\n",
      ".\t16dddJJ\n",
      "8r\t16eee-LL\n",
      ".\t16ggg-\n",
      "*clefF4\t*\n",
      "8a\t16fff\n",
      ".\t16eee-JJ\n",
      "4b-\t16dddLL\n",
      ".\t16eee-\n",
      ".\t16ddd\n",
      ".\t16ccc-JJ\n",
      "8r\t16bb-LL\n",
      ".\t16aa-\n",
      "8d\t16gg-\n",
      ".\t16ffJJ\n",
      "=\t=\n",
      "*-\t*-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e23565256e49a7a47d9e5954eb6707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor=config.experiment.metric_to_watch, min_delta=0.01, patience=2, mode=\"min\", verbose=True)\n",
    "wandb_logger = WandbLogger(project='FP_SMT', group=f\"SMTppNEXT\", name=f\"GrandStaff\", log_model=True)\n",
    "trainer = Trainer(max_epochs=config.experiment.max_epochs, \n",
    "                    check_val_every_n_epoch=config.experiment.val_after, callbacks=[early_stopping], logger=wandb_logger,\n",
    "                    precision='16-mixed')\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# model = SMT.load_from_checkpoint(checkpointer.best_model_path)\n",
    "\n",
    "# trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'MixedPrecision', 'hparams_name', 'hyper_parameters'])\n",
      "Model State Dict Keys: odict_keys(['encoder.downsample_layers.0.0.weight', 'encoder.downsample_layers.0.0.bias', 'encoder.downsample_layers.0.1.weight', 'encoder.downsample_layers.0.1.bias', 'encoder.downsample_layers.1.0.weight', 'encoder.downsample_layers.1.0.bias', 'encoder.downsample_layers.1.1.weight', 'encoder.downsample_layers.1.1.bias', 'encoder.downsample_layers.2.0.weight', 'encoder.downsample_layers.2.0.bias', 'encoder.downsample_layers.2.1.weight', 'encoder.downsample_layers.2.1.bias', 'encoder.stages.0.0.gamma', 'encoder.stages.0.0.dwconv.weight', 'encoder.stages.0.0.dwconv.bias', 'encoder.stages.0.0.norm.weight', 'encoder.stages.0.0.norm.bias', 'encoder.stages.0.0.pwconv1.weight', 'encoder.stages.0.0.pwconv1.bias', 'encoder.stages.0.0.pwconv2.weight', 'encoder.stages.0.0.pwconv2.bias', 'encoder.stages.0.1.gamma', 'encoder.stages.0.1.dwconv.weight', 'encoder.stages.0.1.dwconv.bias', 'encoder.stages.0.1.norm.weight', 'encoder.stages.0.1.norm.bias', 'encoder.stages.0.1.pwconv1.weight', 'encoder.stages.0.1.pwconv1.bias', 'encoder.stages.0.1.pwconv2.weight', 'encoder.stages.0.1.pwconv2.bias', 'encoder.stages.0.2.gamma', 'encoder.stages.0.2.dwconv.weight', 'encoder.stages.0.2.dwconv.bias', 'encoder.stages.0.2.norm.weight', 'encoder.stages.0.2.norm.bias', 'encoder.stages.0.2.pwconv1.weight', 'encoder.stages.0.2.pwconv1.bias', 'encoder.stages.0.2.pwconv2.weight', 'encoder.stages.0.2.pwconv2.bias', 'encoder.stages.1.0.gamma', 'encoder.stages.1.0.dwconv.weight', 'encoder.stages.1.0.dwconv.bias', 'encoder.stages.1.0.norm.weight', 'encoder.stages.1.0.norm.bias', 'encoder.stages.1.0.pwconv1.weight', 'encoder.stages.1.0.pwconv1.bias', 'encoder.stages.1.0.pwconv2.weight', 'encoder.stages.1.0.pwconv2.bias', 'encoder.stages.1.1.gamma', 'encoder.stages.1.1.dwconv.weight', 'encoder.stages.1.1.dwconv.bias', 'encoder.stages.1.1.norm.weight', 'encoder.stages.1.1.norm.bias', 'encoder.stages.1.1.pwconv1.weight', 'encoder.stages.1.1.pwconv1.bias', 'encoder.stages.1.1.pwconv2.weight', 'encoder.stages.1.1.pwconv2.bias', 'encoder.stages.1.2.gamma', 'encoder.stages.1.2.dwconv.weight', 'encoder.stages.1.2.dwconv.bias', 'encoder.stages.1.2.norm.weight', 'encoder.stages.1.2.norm.bias', 'encoder.stages.1.2.pwconv1.weight', 'encoder.stages.1.2.pwconv1.bias', 'encoder.stages.1.2.pwconv2.weight', 'encoder.stages.1.2.pwconv2.bias', 'encoder.stages.2.0.gamma', 'encoder.stages.2.0.dwconv.weight', 'encoder.stages.2.0.dwconv.bias', 'encoder.stages.2.0.norm.weight', 'encoder.stages.2.0.norm.bias', 'encoder.stages.2.0.pwconv1.weight', 'encoder.stages.2.0.pwconv1.bias', 'encoder.stages.2.0.pwconv2.weight', 'encoder.stages.2.0.pwconv2.bias', 'encoder.stages.2.1.gamma', 'encoder.stages.2.1.dwconv.weight', 'encoder.stages.2.1.dwconv.bias', 'encoder.stages.2.1.norm.weight', 'encoder.stages.2.1.norm.bias', 'encoder.stages.2.1.pwconv1.weight', 'encoder.stages.2.1.pwconv1.bias', 'encoder.stages.2.1.pwconv2.weight', 'encoder.stages.2.1.pwconv2.bias', 'encoder.stages.2.2.gamma', 'encoder.stages.2.2.dwconv.weight', 'encoder.stages.2.2.dwconv.bias', 'encoder.stages.2.2.norm.weight', 'encoder.stages.2.2.norm.bias', 'encoder.stages.2.2.pwconv1.weight', 'encoder.stages.2.2.pwconv1.bias', 'encoder.stages.2.2.pwconv2.weight', 'encoder.stages.2.2.pwconv2.bias', 'encoder.stages.2.3.gamma', 'encoder.stages.2.3.dwconv.weight', 'encoder.stages.2.3.dwconv.bias', 'encoder.stages.2.3.norm.weight', 'encoder.stages.2.3.norm.bias', 'encoder.stages.2.3.pwconv1.weight', 'encoder.stages.2.3.pwconv1.bias', 'encoder.stages.2.3.pwconv2.weight', 'encoder.stages.2.3.pwconv2.bias', 'encoder.stages.2.4.gamma', 'encoder.stages.2.4.dwconv.weight', 'encoder.stages.2.4.dwconv.bias', 'encoder.stages.2.4.norm.weight', 'encoder.stages.2.4.norm.bias', 'encoder.stages.2.4.pwconv1.weight', 'encoder.stages.2.4.pwconv1.bias', 'encoder.stages.2.4.pwconv2.weight', 'encoder.stages.2.4.pwconv2.bias', 'encoder.stages.2.5.gamma', 'encoder.stages.2.5.dwconv.weight', 'encoder.stages.2.5.dwconv.bias', 'encoder.stages.2.5.norm.weight', 'encoder.stages.2.5.norm.bias', 'encoder.stages.2.5.pwconv1.weight', 'encoder.stages.2.5.pwconv1.bias', 'encoder.stages.2.5.pwconv2.weight', 'encoder.stages.2.5.pwconv2.bias', 'encoder.stages.2.6.gamma', 'encoder.stages.2.6.dwconv.weight', 'encoder.stages.2.6.dwconv.bias', 'encoder.stages.2.6.norm.weight', 'encoder.stages.2.6.norm.bias', 'encoder.stages.2.6.pwconv1.weight', 'encoder.stages.2.6.pwconv1.bias', 'encoder.stages.2.6.pwconv2.weight', 'encoder.stages.2.6.pwconv2.bias', 'encoder.stages.2.7.gamma', 'encoder.stages.2.7.dwconv.weight', 'encoder.stages.2.7.dwconv.bias', 'encoder.stages.2.7.norm.weight', 'encoder.stages.2.7.norm.bias', 'encoder.stages.2.7.pwconv1.weight', 'encoder.stages.2.7.pwconv1.bias', 'encoder.stages.2.7.pwconv2.weight', 'encoder.stages.2.7.pwconv2.bias', 'encoder.stages.2.8.gamma', 'encoder.stages.2.8.dwconv.weight', 'encoder.stages.2.8.dwconv.bias', 'encoder.stages.2.8.norm.weight', 'encoder.stages.2.8.norm.bias', 'encoder.stages.2.8.pwconv1.weight', 'encoder.stages.2.8.pwconv1.bias', 'encoder.stages.2.8.pwconv2.weight', 'encoder.stages.2.8.pwconv2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.decoder.layers.0.input_attention.lq.weight', 'decoder.decoder.layers.0.input_attention.lq.bias', 'decoder.decoder.layers.0.input_attention.lk.weight', 'decoder.decoder.layers.0.input_attention.lk.bias', 'decoder.decoder.layers.0.input_attention.lv.weight', 'decoder.decoder.layers.0.input_attention.lv.bias', 'decoder.decoder.layers.0.input_attention.out_proj.weight', 'decoder.decoder.layers.0.input_attention.out_proj.bias', 'decoder.decoder.layers.0.norm1.weight', 'decoder.decoder.layers.0.norm1.bias', 'decoder.decoder.layers.0.cross_attention.lq.weight', 'decoder.decoder.layers.0.cross_attention.lq.bias', 'decoder.decoder.layers.0.cross_attention.lk.weight', 'decoder.decoder.layers.0.cross_attention.lk.bias', 'decoder.decoder.layers.0.cross_attention.lv.weight', 'decoder.decoder.layers.0.cross_attention.lv.bias', 'decoder.decoder.layers.0.cross_attention.out_proj.weight', 'decoder.decoder.layers.0.cross_attention.out_proj.bias', 'decoder.decoder.layers.0.ffNet.0.weight', 'decoder.decoder.layers.0.ffNet.0.bias', 'decoder.decoder.layers.0.ffNet.3.weight', 'decoder.decoder.layers.0.ffNet.3.bias', 'decoder.decoder.layers.0.norm2.weight', 'decoder.decoder.layers.0.norm2.bias', 'decoder.decoder.layers.0.norm3.weight', 'decoder.decoder.layers.0.norm3.bias', 'decoder.decoder.layers.1.input_attention.lq.weight', 'decoder.decoder.layers.1.input_attention.lq.bias', 'decoder.decoder.layers.1.input_attention.lk.weight', 'decoder.decoder.layers.1.input_attention.lk.bias', 'decoder.decoder.layers.1.input_attention.lv.weight', 'decoder.decoder.layers.1.input_attention.lv.bias', 'decoder.decoder.layers.1.input_attention.out_proj.weight', 'decoder.decoder.layers.1.input_attention.out_proj.bias', 'decoder.decoder.layers.1.norm1.weight', 'decoder.decoder.layers.1.norm1.bias', 'decoder.decoder.layers.1.cross_attention.lq.weight', 'decoder.decoder.layers.1.cross_attention.lq.bias', 'decoder.decoder.layers.1.cross_attention.lk.weight', 'decoder.decoder.layers.1.cross_attention.lk.bias', 'decoder.decoder.layers.1.cross_attention.lv.weight', 'decoder.decoder.layers.1.cross_attention.lv.bias', 'decoder.decoder.layers.1.cross_attention.out_proj.weight', 'decoder.decoder.layers.1.cross_attention.out_proj.bias', 'decoder.decoder.layers.1.ffNet.0.weight', 'decoder.decoder.layers.1.ffNet.0.bias', 'decoder.decoder.layers.1.ffNet.3.weight', 'decoder.decoder.layers.1.ffNet.3.bias', 'decoder.decoder.layers.1.norm2.weight', 'decoder.decoder.layers.1.norm2.bias', 'decoder.decoder.layers.1.norm3.weight', 'decoder.decoder.layers.1.norm3.bias', 'decoder.decoder.layers.2.input_attention.lq.weight', 'decoder.decoder.layers.2.input_attention.lq.bias', 'decoder.decoder.layers.2.input_attention.lk.weight', 'decoder.decoder.layers.2.input_attention.lk.bias', 'decoder.decoder.layers.2.input_attention.lv.weight', 'decoder.decoder.layers.2.input_attention.lv.bias', 'decoder.decoder.layers.2.input_attention.out_proj.weight', 'decoder.decoder.layers.2.input_attention.out_proj.bias', 'decoder.decoder.layers.2.norm1.weight', 'decoder.decoder.layers.2.norm1.bias', 'decoder.decoder.layers.2.cross_attention.lq.weight', 'decoder.decoder.layers.2.cross_attention.lq.bias', 'decoder.decoder.layers.2.cross_attention.lk.weight', 'decoder.decoder.layers.2.cross_attention.lk.bias', 'decoder.decoder.layers.2.cross_attention.lv.weight', 'decoder.decoder.layers.2.cross_attention.lv.bias', 'decoder.decoder.layers.2.cross_attention.out_proj.weight', 'decoder.decoder.layers.2.cross_attention.out_proj.bias', 'decoder.decoder.layers.2.ffNet.0.weight', 'decoder.decoder.layers.2.ffNet.0.bias', 'decoder.decoder.layers.2.ffNet.3.weight', 'decoder.decoder.layers.2.ffNet.3.bias', 'decoder.decoder.layers.2.norm2.weight', 'decoder.decoder.layers.2.norm2.bias', 'decoder.decoder.layers.2.norm3.weight', 'decoder.decoder.layers.2.norm3.bias', 'decoder.decoder.layers.3.input_attention.lq.weight', 'decoder.decoder.layers.3.input_attention.lq.bias', 'decoder.decoder.layers.3.input_attention.lk.weight', 'decoder.decoder.layers.3.input_attention.lk.bias', 'decoder.decoder.layers.3.input_attention.lv.weight', 'decoder.decoder.layers.3.input_attention.lv.bias', 'decoder.decoder.layers.3.input_attention.out_proj.weight', 'decoder.decoder.layers.3.input_attention.out_proj.bias', 'decoder.decoder.layers.3.norm1.weight', 'decoder.decoder.layers.3.norm1.bias', 'decoder.decoder.layers.3.cross_attention.lq.weight', 'decoder.decoder.layers.3.cross_attention.lq.bias', 'decoder.decoder.layers.3.cross_attention.lk.weight', 'decoder.decoder.layers.3.cross_attention.lk.bias', 'decoder.decoder.layers.3.cross_attention.lv.weight', 'decoder.decoder.layers.3.cross_attention.lv.bias', 'decoder.decoder.layers.3.cross_attention.out_proj.weight', 'decoder.decoder.layers.3.cross_attention.out_proj.bias', 'decoder.decoder.layers.3.ffNet.0.weight', 'decoder.decoder.layers.3.ffNet.0.bias', 'decoder.decoder.layers.3.ffNet.3.weight', 'decoder.decoder.layers.3.ffNet.3.bias', 'decoder.decoder.layers.3.norm2.weight', 'decoder.decoder.layers.3.norm2.bias', 'decoder.decoder.layers.3.norm3.weight', 'decoder.decoder.layers.3.norm3.bias', 'decoder.decoder.layers.4.input_attention.lq.weight', 'decoder.decoder.layers.4.input_attention.lq.bias', 'decoder.decoder.layers.4.input_attention.lk.weight', 'decoder.decoder.layers.4.input_attention.lk.bias', 'decoder.decoder.layers.4.input_attention.lv.weight', 'decoder.decoder.layers.4.input_attention.lv.bias', 'decoder.decoder.layers.4.input_attention.out_proj.weight', 'decoder.decoder.layers.4.input_attention.out_proj.bias', 'decoder.decoder.layers.4.norm1.weight', 'decoder.decoder.layers.4.norm1.bias', 'decoder.decoder.layers.4.cross_attention.lq.weight', 'decoder.decoder.layers.4.cross_attention.lq.bias', 'decoder.decoder.layers.4.cross_attention.lk.weight', 'decoder.decoder.layers.4.cross_attention.lk.bias', 'decoder.decoder.layers.4.cross_attention.lv.weight', 'decoder.decoder.layers.4.cross_attention.lv.bias', 'decoder.decoder.layers.4.cross_attention.out_proj.weight', 'decoder.decoder.layers.4.cross_attention.out_proj.bias', 'decoder.decoder.layers.4.ffNet.0.weight', 'decoder.decoder.layers.4.ffNet.0.bias', 'decoder.decoder.layers.4.ffNet.3.weight', 'decoder.decoder.layers.4.ffNet.3.bias', 'decoder.decoder.layers.4.norm2.weight', 'decoder.decoder.layers.4.norm2.bias', 'decoder.decoder.layers.4.norm3.weight', 'decoder.decoder.layers.4.norm3.bias', 'decoder.decoder.layers.5.input_attention.lq.weight', 'decoder.decoder.layers.5.input_attention.lq.bias', 'decoder.decoder.layers.5.input_attention.lk.weight', 'decoder.decoder.layers.5.input_attention.lk.bias', 'decoder.decoder.layers.5.input_attention.lv.weight', 'decoder.decoder.layers.5.input_attention.lv.bias', 'decoder.decoder.layers.5.input_attention.out_proj.weight', 'decoder.decoder.layers.5.input_attention.out_proj.bias', 'decoder.decoder.layers.5.norm1.weight', 'decoder.decoder.layers.5.norm1.bias', 'decoder.decoder.layers.5.cross_attention.lq.weight', 'decoder.decoder.layers.5.cross_attention.lq.bias', 'decoder.decoder.layers.5.cross_attention.lk.weight', 'decoder.decoder.layers.5.cross_attention.lk.bias', 'decoder.decoder.layers.5.cross_attention.lv.weight', 'decoder.decoder.layers.5.cross_attention.lv.bias', 'decoder.decoder.layers.5.cross_attention.out_proj.weight', 'decoder.decoder.layers.5.cross_attention.out_proj.bias', 'decoder.decoder.layers.5.ffNet.0.weight', 'decoder.decoder.layers.5.ffNet.0.bias', 'decoder.decoder.layers.5.ffNet.3.weight', 'decoder.decoder.layers.5.ffNet.3.bias', 'decoder.decoder.layers.5.norm2.weight', 'decoder.decoder.layers.5.norm2.bias', 'decoder.decoder.layers.5.norm3.weight', 'decoder.decoder.layers.5.norm3.bias', 'decoder.decoder.layers.6.input_attention.lq.weight', 'decoder.decoder.layers.6.input_attention.lq.bias', 'decoder.decoder.layers.6.input_attention.lk.weight', 'decoder.decoder.layers.6.input_attention.lk.bias', 'decoder.decoder.layers.6.input_attention.lv.weight', 'decoder.decoder.layers.6.input_attention.lv.bias', 'decoder.decoder.layers.6.input_attention.out_proj.weight', 'decoder.decoder.layers.6.input_attention.out_proj.bias', 'decoder.decoder.layers.6.norm1.weight', 'decoder.decoder.layers.6.norm1.bias', 'decoder.decoder.layers.6.cross_attention.lq.weight', 'decoder.decoder.layers.6.cross_attention.lq.bias', 'decoder.decoder.layers.6.cross_attention.lk.weight', 'decoder.decoder.layers.6.cross_attention.lk.bias', 'decoder.decoder.layers.6.cross_attention.lv.weight', 'decoder.decoder.layers.6.cross_attention.lv.bias', 'decoder.decoder.layers.6.cross_attention.out_proj.weight', 'decoder.decoder.layers.6.cross_attention.out_proj.bias', 'decoder.decoder.layers.6.ffNet.0.weight', 'decoder.decoder.layers.6.ffNet.0.bias', 'decoder.decoder.layers.6.ffNet.3.weight', 'decoder.decoder.layers.6.ffNet.3.bias', 'decoder.decoder.layers.6.norm2.weight', 'decoder.decoder.layers.6.norm2.bias', 'decoder.decoder.layers.6.norm3.weight', 'decoder.decoder.layers.6.norm3.bias', 'decoder.decoder.layers.7.input_attention.lq.weight', 'decoder.decoder.layers.7.input_attention.lq.bias', 'decoder.decoder.layers.7.input_attention.lk.weight', 'decoder.decoder.layers.7.input_attention.lk.bias', 'decoder.decoder.layers.7.input_attention.lv.weight', 'decoder.decoder.layers.7.input_attention.lv.bias', 'decoder.decoder.layers.7.input_attention.out_proj.weight', 'decoder.decoder.layers.7.input_attention.out_proj.bias', 'decoder.decoder.layers.7.norm1.weight', 'decoder.decoder.layers.7.norm1.bias', 'decoder.decoder.layers.7.cross_attention.lq.weight', 'decoder.decoder.layers.7.cross_attention.lq.bias', 'decoder.decoder.layers.7.cross_attention.lk.weight', 'decoder.decoder.layers.7.cross_attention.lk.bias', 'decoder.decoder.layers.7.cross_attention.lv.weight', 'decoder.decoder.layers.7.cross_attention.lv.bias', 'decoder.decoder.layers.7.cross_attention.out_proj.weight', 'decoder.decoder.layers.7.cross_attention.out_proj.bias', 'decoder.decoder.layers.7.ffNet.0.weight', 'decoder.decoder.layers.7.ffNet.0.bias', 'decoder.decoder.layers.7.ffNet.3.weight', 'decoder.decoder.layers.7.ffNet.3.bias', 'decoder.decoder.layers.7.norm2.weight', 'decoder.decoder.layers.7.norm2.bias', 'decoder.decoder.layers.7.norm3.weight', 'decoder.decoder.layers.7.norm3.bias', 'decoder.embedding.weight', 'decoder.out_layer.weight', 'decoder.out_layer.bias'])\n",
      "Epoch: 17\n",
      "Global Step: 87036\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_checkpoint.ckpt\")\n",
    "\n",
    "# Print the keys in the checkpoint to see what information is stored\n",
    "print(checkpoint.keys())\n",
    "model_state_dict = checkpoint['state_dict']\n",
    "optimizer_state_dict = checkpoint['optimizer_states']\n",
    "epoch = checkpoint['epoch']\n",
    "global_step = checkpoint['global_step']\n",
    "\n",
    "print(\"Model State Dict Keys:\", model_state_dict.keys())\n",
    "\n",
    "print(\"Epoch:\", epoch)\n",
    "print(\"Global Step:\", global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at model_checkpoint.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at model_checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acdd232a94d4d019c7543557c6180fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.validate(model=model, datamodule=data_module, ckpt_path=\"model_checkpoint.ckpt\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
